{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a247ea0",
   "metadata": {},
   "source": [
    "# Setup Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea02dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism, ensure_tuple\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandAffined,\n",
    "    RandRotate90d,\n",
    "    RandShiftIntensityd,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    RandAdjustContrastd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    AddChanneld,\n",
    "    RandGaussianSharpend,\n",
    "    RandGaussianSmoothd,\n",
    "    RandHistogramShiftd,\n",
    "    OneOf,\n",
    "    Rand3DElasticd,\n",
    "    Rand3DElastic,\n",
    "    RandGridDistortiond,\n",
    "    RandSpatialCropSamplesd,\n",
    "    FillHoles,\n",
    "    LabelFilter,\n",
    "    LabelToContour\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.losses import DiceLoss, DiceCELoss, DiceFocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch, ImageReader\n",
    "from monai.data.image_reader import WSIReader\n",
    "from monai.config import print_config, KeysCollection, PathLike\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "from torchio.transforms import (\n",
    "    RandomAffine\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from numpy import random\n",
    "from pathlib import Path\n",
    "import re\n",
    "from skimage import io\n",
    "from typing import Optional, Union, Sequence, Callable, Dict, List\n",
    "from monai.data.utils import is_supported_format\n",
    "from monai. data.image_reader import _copy_compatible_dict, _stack_images\n",
    "from nibabel.nifti1 import Nifti1Image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "#from mlflow import log_metric, log_param, log_artifacts, set_experiment, start_run, end_run\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7709d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.0\n",
      "Numpy version: 1.19.5\n",
      "Pytorch version: 1.7.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 714d00dffe6653e21260160666c4c201ab66511b\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.8\n",
      "Nibabel version: 3.2.2\n",
      "scikit-image version: 0.19.1\n",
      "Pillow version: 8.4.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.8.2\n",
      "tqdm version: 4.62.3\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pandas version: 1.3.5\n",
      "einops version: 0.4.0\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 1.23.1\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a98315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIFFReader(ImageReader):\n",
    "    \n",
    "    def __init__(self, npz_keys: Optional[KeysCollection] = None, channel_dim: Optional[int] = None, **kwargs):\n",
    "        super().__init__()\n",
    "        if npz_keys is not None:\n",
    "            npz_keys = ensure_tuple(npz_keys)\n",
    "        self.npz_keys = npz_keys\n",
    "        self.channel_dim = channel_dim\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def verify_suffix(self, filename: Union[Sequence[PathLike], PathLike]) -> bool:\n",
    "        \"\"\"\n",
    "        Verify whether the specified file or files format is supported by Numpy reader.\n",
    "\n",
    "        Args:\n",
    "            filename: file name or a list of file names to read.\n",
    "                if a list of files, verify all the suffixes.\n",
    "        \"\"\"\n",
    "        suffixes: Sequence[str] = [\"tif\", \"tiff\"]\n",
    "        return is_supported_format(filename, suffixes)\n",
    "\n",
    "    def read(self, data: Union[Sequence[PathLike], PathLike], **kwargs):\n",
    "        \"\"\"\n",
    "        Read image data from specified file or files, it can read a list of `no-channel` data files\n",
    "        and stack them together as multi-channels data in `get_data()`.\n",
    "        Note that the returned object is Numpy array or list of Numpy arrays.\n",
    "\n",
    "        Args:\n",
    "            data: file name or a list of file names to read.\n",
    "            kwargs: additional args for `numpy.load` API except `allow_pickle`, will override `self.kwargs` for existing keys.\n",
    "                More details about available args:\n",
    "                https://numpy.org/doc/stable/reference/generated/numpy.load.html\n",
    "\n",
    "        \"\"\"\n",
    "        img_: List[Nifti1Image] = []\n",
    "\n",
    "        filenames: Sequence[PathLike] = ensure_tuple(data)\n",
    "        kwargs_ = self.kwargs.copy()\n",
    "        kwargs_.update(kwargs)\n",
    "        for name in filenames:\n",
    "            img = io.imread(name, **kwargs_)\n",
    "            img = img.astype('float32')\n",
    "            if len(img.shape)==4:\n",
    "                img = np.swapaxes(img,0,1)\n",
    "                img = np.swapaxes(img,1,3)\n",
    "            img_.append(img)\n",
    "        return img_ if len(img_) > 1 else img_[0]\n",
    "    \n",
    "    def get_data(self, img):\n",
    "        \"\"\"\n",
    "        Extract data array and meta data from loaded image and return them.\n",
    "        This function returns two objects, first is numpy array of image data, second is dict of meta data.\n",
    "        It constructs `affine`, `original_affine`, and `spatial_shape` and stores them in meta dict.\n",
    "        When loading a list of files, they are stacked together at a new dimension as the first dimension,\n",
    "        and the meta data of the first image is used to represent the output meta data.\n",
    "\n",
    "        Args:\n",
    "            img: a Numpy array loaded from a file or a list of Numpy arrays.\n",
    "\n",
    "        \"\"\"\n",
    "        img_array: List[np.ndarray] = []\n",
    "        compatible_meta: Dict = {}\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = (img,)\n",
    "\n",
    "        for i in ensure_tuple(img):\n",
    "            header = {\"affine\":np.eye(5),\n",
    "                     \"labels\": {\"0\": \"background\",\n",
    "                                \"1\": \"vessels\",\n",
    "                                \"2\": \"neurons\",}\n",
    "                     }\n",
    "            if isinstance(i, np.ndarray):\n",
    "                # if `channel_dim` is None, can not detect the channel dim, use all the dims as spatial_shape\n",
    "                spatial_shape = np.asarray(i.shape)\n",
    "                if isinstance(self.channel_dim, int):\n",
    "                    spatial_shape = np.delete(spatial_shape, self.channel_dim)\n",
    "                header[\"spatial_shape\"] = spatial_shape\n",
    "            img_array.append(i)\n",
    "            header[\"original_channel_dim\"] = self.channel_dim if isinstance(self.channel_dim, int) else \"no_channel\"\n",
    "            _copy_compatible_dict(header, compatible_meta)\n",
    "\n",
    "        return _stack_images(img_array, compatible_meta), compatible_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db1586",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd15ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_file = 'hyperparameter_pickle_files/parameters58.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9edd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = re.sub('.pickle',\n",
    "                    '',\n",
    "                    re.sub('hyperparameter_pickle_files/parameters',\n",
    "                           '',\n",
    "                           parameter_file\n",
    "                          )\n",
    "                   )\n",
    "#set_experiment(\"TBI_UNet_adam_optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a34ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(parameter_file, 'rb') as handle:\n",
    "    params = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f94a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in params.keys():\n",
    "#    log_param(i,params[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85ed99",
   "metadata": {},
   "source": [
    "# setup directory to save enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae47144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'training_models/58'\n",
      "training_models/58\n"
     ]
    }
   ],
   "source": [
    "directory = re.sub('.pickle',\n",
    "                   '',\n",
    "                   re.sub('hyperparameter_pickle_files/parameters',\n",
    "                          'training_models/',\n",
    "                           parameter_file\n",
    "                         )\n",
    "                  )\n",
    "try:\n",
    "    os.mkdir(directory)\n",
    "except OSError as error:\n",
    "    print(error) \n",
    "#log_artifacts(directory)\n",
    "print(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529c608",
   "metadata": {},
   "source": [
    "# get train and validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of data dictionaries\n",
    "train_images_path = Path('../TBI/GT_filtered+raw/') #raw path images\n",
    "train_images_paths = list(train_images_path.glob('*.tif'))#get images\n",
    "train_images = sorted([x.as_posix() for x in train_images_paths])#sort\n",
    "train_labels_path = Path('../TBI/GT_filtered+raw/')#labels path\n",
    "train_labels = list(train_labels_path.glob('*sub1.tiff'))#get label images\n",
    "train_labels = sorted([x.as_posix() for x in train_labels])#sort\n",
    "#combine images and labels into monai dictionary format\n",
    "data_dicts = [\n",
    "    {\"image\":image_name, \"label\":label_name}\n",
    "    for image_name, label_name in zip(train_images,train_labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_ids_path = Path('../TBI/raw')#each mouse has its own folder with raw data in it\n",
    "mouse_ids = list(mouse_ids_path.glob('*'))#grab molder names/mouse ids\n",
    "images = sorted([y.name for y in train_images_paths])#sort\n",
    "#get mouse id corresponding to each image i have labels for\n",
    "mouse_ids_with_raw_tiff = []\n",
    "for i in mouse_ids:\n",
    "    for j in images:\n",
    "        if len(list(i.glob(j))) !=0:\n",
    "            mouse_ids_with_raw_tiff.append(list(i.glob(j)))\n",
    "#flatten the list and sort\n",
    "mouse_ids_with_raw_tiff_flat = [item for sublist in mouse_ids_with_raw_tiff for item in sublist]\n",
    "mouse_ids_with_raw_tiff_flat = sorted(mouse_ids_with_raw_tiff_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf680925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle mouse ids for a 15/4/6 split train/val/test by mouse id\n",
    "mouse_ids_present = [i.parent.name for i in mouse_ids_with_raw_tiff_flat]\n",
    "mouse_ids_present = sorted(list(np.unique(mouse_ids_present)))\n",
    "np.random.seed(643)\n",
    "np.random.shuffle(mouse_ids_present)\n",
    "mouse_ids_present\n",
    "train = mouse_ids_present[:15]\n",
    "#log_param('train_set',' '.join(train))\n",
    "val = mouse_ids_present[15:-6]\n",
    "#log_param('val_set',' '.join(val))\n",
    "test = mouse_ids_present[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36479b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "for i in train:\n",
    "    for j in mouse_ids_with_raw_tiff_flat:\n",
    "        if i in j.as_posix():\n",
    "            for k in data_dicts:\n",
    "                if j.name in k[\"image\"]:\n",
    "                    train_files.append(k)\n",
    "for i in val:\n",
    "    for j in mouse_ids_with_raw_tiff_flat:\n",
    "        if i in j.as_posix():\n",
    "            for k in data_dicts:\n",
    "                if j.name in k[\"image\"]:\n",
    "                    val_files.append(k)\n",
    "for i in test:\n",
    "    for j in mouse_ids_with_raw_tiff_flat:\n",
    "        if i in j.as_posix():\n",
    "            for k in data_dicts:\n",
    "                if j.name in k[\"image\"]:\n",
    "                    test_files.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57ca85",
   "metadata": {},
   "source": [
    "# Set deterministic training for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e78a1",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation\n",
    "\n",
    "Here we use several transforms to augment the dataset:\n",
    "1. `LoadImaged` loads the spleen CT images and labels from NIfTI format files.\n",
    "1. `AddChanneld` as the original data doesn't have channel dim, add 1 dim to construct \"channel first\" shape.\n",
    "1. `Spacingd` adjusts the spacing by `pixdim=(1.5, 1.5, 2.)` based on the affine matrix.\n",
    "1. `Orientationd` unifies the data orientation based on the affine matrix.\n",
    "1. `ScaleIntensityRanged` extracts intensity range [-57, 164] and scales to [0, 1].\n",
    "1. `CropForegroundd` removes all zero borders to focus on the valid body area of the images and labels.\n",
    "1. `RandCropByPosNegLabeld` randomly crop patch samples from big image based on pos / neg ratio.  \n",
    "The image centers of negative samples must be in valid body area.\n",
    "1. `RandAffined` efficiently performs `rotate`, `scale`, `shear`, `translate`, etc. together based on PyTorch affine transform.\n",
    "1. `EnsureTyped` converts the numpy array to PyTorch Tensor for further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], reader = TIFFReader(channel_dim = 0)),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
    "            1.01, 1.01, 0.3787), mode=(\"bilinear\", \"nearest\")),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=0, a_max=1024,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        AsDiscreted(keys=\"label\",\n",
    "                   rounding='torchrounding',\n",
    "                   to_onehot=True),\n",
    "        #random cropping\n",
    "        RandSpatialCropSamplesd(\n",
    "            keys=['image','label'],\n",
    "            num_samples = params['N_crops'],\n",
    "            roi_size = params['crop_size'],\n",
    "            random_size = False\n",
    "        ),\n",
    "        #Deformation_transforms\n",
    "        OneOf(transforms=[Rand3DElasticd(keys = [\"image\",\"label\"],\n",
    "                                        sigma_range = params['Rand3DElasticd_sigma_range'],\n",
    "                                        magnitude_range = params['Rand3DElasticd_magnitude_range'],\n",
    "                                        prob = params[\"deformation_transforms_prob\"],\n",
    "                                        mode = [\"bilinear\",\"nearest\"]),\n",
    "                          RandGridDistortiond(keys = [\"image\",\"label\"],\n",
    "                                             num_cells = params['RandGridDistortiond_num_cells'],\n",
    "                                             prob = params[\"deformation_transforms_prob\"],\n",
    "                                             distort_limit = params['RandGridDistortiond_distort_limit'],\n",
    "                                             mode = [\"bilinear\",\"nearest\"]\n",
    "                                             )\n",
    "                         ]\n",
    "             ),\n",
    "        #Intensity_Transforms\n",
    "        OneOf(transforms = [RandShiftIntensityd(keys = [\"image\"],\n",
    "                                                offsets = params['RandShiftIntensityd_offsets'],\n",
    "                                                prob = params[\"intensity_transform_probability\"]),\n",
    "                            RandAdjustContrastd(keys = [\"image\"],\n",
    "                                                prob = params[\"intensity_transform_probability\"],\n",
    "                                                gamma = params['RandAdjustContrastd_gamma']),\n",
    "                            RandHistogramShiftd(keys = [\"image\"],\n",
    "                                                prob = params[\"intensity_transform_probability\"],\n",
    "                                                num_control_points = params['RandHistogramShiftd_num_control_points'])\n",
    "                           ]\n",
    "             ),\n",
    "        #Gaussian_Transforms\n",
    "        OneOf(transforms = [RandGaussianSharpend(keys = [\"image\"],\n",
    "                                                 prob = params[\"gaussian_transform_probability\"]),\n",
    "                            RandGaussianSmoothd(keys = [\"image\"],\n",
    "                                                prob = params[\"gaussian_transform_probability\"]),\n",
    "                            RandGaussianNoised(keys = [\"image\"],\n",
    "                                               prob = params[\"gaussian_transform_probability\"],\n",
    "                                               mean = params['RandGaussianNoised_mean'],\n",
    "                                               std = params['RandGaussianNoised_std'])\n",
    "                           ]\n",
    "             ),\n",
    "        #rottion+flip_transforms\n",
    "        RandRotate90d(\n",
    "            keys = [\"image\", \"label\"],\n",
    "            prob = params['rotation_flip_transforms_probability'],\n",
    "            max_k = 3,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys = [\"image\", \"label\"],\n",
    "            spatial_axis = [0],\n",
    "            prob = params['rotation_flip_transforms_probability'],\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys = [\"image\", \"label\"],\n",
    "            spatial_axis = [1],\n",
    "            prob = params['rotation_flip_transforms_probability'],\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys = [\"image\", \"label\"],\n",
    "            spatial_axis = [2],\n",
    "            prob = params['rotation_flip_transforms_probability'],\n",
    "        ),\n",
    "        #random affine\n",
    "        RandomAffine(\n",
    "            include=[\"image\", \"label\"], \n",
    "            p=params[\"RandomAffine_probability\"], \n",
    "            degrees=params[\"RandomAffine_degrees\"], \n",
    "            scales=params[\"RandomAffine_scales\"],\n",
    "            translation=params[\"RandomAffine_translation\"],\n",
    "            center='image'\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"],reader = TIFFReader(channel_dim = 0)),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
    "            1.01, 1.01, 0.3787), mode=(\"bilinear\", \"nearest\")),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=0, a_max=1024,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        AsDiscreted(keys=\"label\",\n",
    "                   rounding='torchrounding',\n",
    "                   to_onehot=True),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#val_transforms_deform = Compose(\n",
    "#    [\n",
    "#        LoadImaged(keys=[\"image\", \"label\"],reader = TIFFReader(channel_dim = 0)),\n",
    "#        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "#        Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
    "#            1.01, 1.01, 0.3787), mode=(\"bilinear\", \"nearest\")),\n",
    "#        ScaleIntensityRanged(\n",
    "#            keys=[\"image\"], a_min=0, a_max=1024,\n",
    "#            b_min=0.0, b_max=1.0, clip=True,\n",
    "#        ),\n",
    "#        AsDiscreted(keys=\"label\",\n",
    "#                   rounding='torchrounding',\n",
    "#                   to_onehot=True),\n",
    "#        Rand3DElasticd(keys = [\"image\",\"label\"],\n",
    "#                              sigma_range = (1,2),\n",
    "#                              magnitude_range = (3,4),\n",
    "#                              prob = 1,\n",
    "#                              mode = [\"bilinear\",\"nearest\"]\n",
    "#                      ),\n",
    "#        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104ebdd",
   "metadata": {},
   "source": [
    "# Check transforms in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae63422",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image, label = (check_data[\"image\"][0], check_data[\"label\"][0])\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "# plot the slice [ :, 80]\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "img2 = torch.swapaxes(image[:,:, :, 80],0,2)\n",
    "img2 = img2.cpu().detach().numpy()\n",
    "img2 = np.append(img2,np.zeros((507,507,1)), axis=2)/np.max(img2)\n",
    "plt.imshow(img2, cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "img3 = torch.swapaxes(label[:,:, :, 80],0,2)\n",
    "img3 = img3.cpu().detach().numpy()\n",
    "plt.imshow(img3)\n",
    "plt.show()\n",
    "\n",
    "#check_ds = Dataset(data=val_files, transform=val_transforms_deform)\n",
    "#check_loader = DataLoader(check_ds, batch_size=1)\n",
    "#check_data = first(check_loader)\n",
    "#image2, label2 = (check_data[\"image\"][0], check_data[\"label\"][0])\n",
    "#print(f\"image shape: {image2.shape}, label shape: {label2.shape}\")\n",
    "## plot the slice [ :, 80]\n",
    "#plt.figure(\"check\", (12, 6))\n",
    "#plt.subplot(1, 2, 1)\n",
    "#plt.title(\"image\")\n",
    "#img2 = torch.swapaxes(image2[:,:, :, 80],0,2)\n",
    "#img2 = img2.cpu().detach().numpy()\n",
    "#img2 = np.append(img2,np.zeros((507,507,1)), axis=2)/np.max(img2)\n",
    "#plt.imshow(img2, cmap=\"gray\")\n",
    "#plt.subplot(1, 2, 2)\n",
    "#plt.title(\"label\")\n",
    "#img3 = torch.swapaxes(label2[:,:, :, 80],0,2)\n",
    "#img3 = img3.cpu().detach().numpy()\n",
    "#plt.imshow(img3)\n",
    "#plt.show()\n",
    "#\n",
    "#post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "#\n",
    "#label = post_label(label)\n",
    "#label2 = post_label(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e743617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_filter = Compose([EnsureType(), LabelFilter(applied_labels=(1))])\n",
    "#\n",
    "#dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "#label = label_filter(label)\n",
    "#label2 = label_filter(label2)\n",
    "#contour = LabelToContour()(label_filter(label))\n",
    "##print(contour.shape)\n",
    "#contour2 = LabelToContour()(label_filter(label2))\n",
    "#dice_metric(y_pred=contour, y=contour2)\n",
    "#metric = dice_metric.aggregate().item()\n",
    "#dice_metric.reset()\n",
    "#print(metric)\n",
    "#print(\"boundary difference\",torch.sum(contour2.ne(contour).long()))\n",
    "#print(\"total boundary size\",torch.sum(contour2.ne(contour).long())/torch.sum(contour)*100)\n",
    "#label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958bed12",
   "metadata": {},
   "source": [
    "## Define CacheDataset and DataLoader for training and validation\n",
    "\n",
    "Here we use CacheDataset to accelerate training and validation process, it's 10x faster than the regular Dataset.  \n",
    "To achieve best performance, set `cache_rate=1.0` to cache all the data, if memory is not enough, set lower value.  \n",
    "Users can also set `cache_num` instead of `cache_rate`, will use the minimum value of the 2 settings.  \n",
    "And set `num_workers` to enable multi-threads during caching.  \n",
    "If want to to try the regular Dataset, just change to use the commented code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d151d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CacheDataset(\n",
    "    data=train_files, transform=train_transforms,\n",
    "    cache_rate=1.0, num_workers=4)\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "# to generate 2 x 4 images for network training\n",
    "train_loader = DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = CacheDataset(\n",
    "    data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "#val_ds_deform = CacheDataset(\n",
    "#    data=val_files, transform=val_transforms_deform, cache_rate=1.0, num_workers=4)\n",
    "#val_loader_deform = DataLoader(val_ds_deform, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594f58c",
   "metadata": {},
   "source": [
    "# Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "#device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=2,\n",
    "    out_channels=3,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=params['num_res_units'],\n",
    "    norm=params[\"norm\"],\n",
    "    dropout=params[\"dropout\"]\n",
    ")\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "loss_function = params['loss_function']\n",
    "optimizer = params['optimizer'](params = model.parameters(), \n",
    "                                lr = params['learning_rate'])\n",
    "dice_metric = DiceMetric(include_background=False, \n",
    "                         reduction=\"mean\")\n",
    "#hausdorf_distance_metric = HausdorffDistanceMetric(include_background=False,\n",
    "#                                                   distance_metric='euclidean')\n",
    "dice_metric_deform = DiceMetric(include_background=False,\n",
    "                                reduction=\"mean\")\n",
    "dice_metric_deform_boundary_difference = DiceMetric(include_background=False,\n",
    "                                                    reduction=\"mean\")\n",
    "dice_metric_predicted_deform_boundary_difference = DiceMetric(include_background=False,\n",
    "                                                              reduction=\"mean\")\n",
    "dice_metric_boundary_difference_detection = DiceMetric(include_background=False,\n",
    "                                                       reduction=\"mean\")\n",
    "label_filter = Compose(\n",
    "    [EnsureType(),\n",
    "     LabelFilter(applied_labels=(1))\n",
    "    ]\n",
    ")\n",
    "deform = Rand3DElastic(\n",
    "    sigma_range = (1,2),\n",
    "    magnitude_range = (3,4),\n",
    "    prob = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee82911",
   "metadata": {},
   "source": [
    "# Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d838c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12)\n",
    "max_epochs = params[\"max_epochs\"]\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "#hausdorf_distance_values = []\n",
    "metric_values_deform = []\n",
    "metric_values_deform_boundary_difference = []\n",
    "metric_values_predicted_deform_boundary_difference = []\n",
    "metric_values_boundary_difference_detection = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True,to_onehot=3)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    #log_metric('epoch_loss',epoch_loss, step = epoch)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                seed = random.randint(0,10000000)\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                #val_inputs_deform, val_labels_deform = (\n",
    "                #    val_data_deform[\"image\"].to(device),\n",
    "                #    val_data_deform[\"label\"].to(device),\n",
    "                #)\n",
    "                roi_size = (128, 128, 128)\n",
    "                sw_batch_size = 1\n",
    "                val_outputs = sliding_window_inference(\n",
    "                    val_inputs, \n",
    "                    roi_size, \n",
    "                    sw_batch_size, \n",
    "                    model\n",
    "                )\n",
    "                #get prediciton output\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                #deform raw image\n",
    "                deform.set_random_state(seed = seed)\n",
    "                deform_val_inputs = torch.unsqueeze(deform(torch.squeeze(val_inputs),mode='bilinear'),axis=0)\n",
    "                #deform validation output\n",
    "                deform.set_random_state(seed = seed)\n",
    "                deform_val_outputs_gt = deform(val_outputs[0],mode='nearest')\n",
    "                #predict on deformed raw image\n",
    "                deform_val_outputs =sliding_window_inference(\n",
    "                    deform_val_inputs, \n",
    "                    roi_size, \n",
    "                    sw_batch_size, \n",
    "                    model\n",
    "                )   \n",
    "                # get predicted outputs\n",
    "                val_outputs_deform = [post_pred(i) for i in decollate_batch(deform_val_outputs)]\n",
    "                #fill holes in prediction\n",
    "                filled  = [FillHoles(applied_labels=[1],connectivity=2)(i) for i in val_outputs]\n",
    "                filled_deform  = [FillHoles(applied_labels=[1],connectivity=2)(i) for i in val_outputs_deform]\n",
    "                #generated boundaries from predictions\n",
    "                filled_vessels_boundary = [LabelToContour()(label_filter(i)) for i in filled]\n",
    "                filled_vessels_boundary_deform = [LabelToContour()(label_filter(i)) for i in filled_deform]\n",
    "                #get labelled data\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                #print(val_labels.shape)\n",
    "                val_labels_deform = [deform_val_outputs_gt]\n",
    "                #get validation boundaries\n",
    "                val_labels_boundary = [LabelToContour()(i) for i in val_labels]\n",
    "                val_outputs_boundary_deform = [LabelToContour()(i) for i in val_labels_deform]\n",
    "                # compute metric for current iteration\n",
    "                #dice metric for ground truth and prediction\n",
    "                dice_metric(\n",
    "                    y_pred = filled,\n",
    "                    y = val_labels\n",
    "                )\n",
    "                #hausdorf_distance_metric(y_pred=filled, \n",
    "                #                         y=val_labels)\n",
    "                #dice metric for deformed output, and prediction on deformed raw image\n",
    "                dice_metric_deform(\n",
    "                    y_pred = filled_deform,\n",
    "                    y = val_outputs_deform\n",
    "                )\n",
    "                #dice metric for boundary of ground truth and deformed ground truth\n",
    "                #measures how much the boundary was deformed\n",
    "                dice_metric_deform_boundary_difference(\n",
    "                    y_pred = val_outputs_boundary_deform,\n",
    "                    y = filled_vessels_boundary\n",
    "                )\n",
    "                \n",
    "                dice_metric_predicted_deform_boundary_difference(\n",
    "                    y_pred=val_outputs_boundary_deform[0],\n",
    "                    y=filled_vessels_boundary_deform[0]\n",
    "                )\n",
    "                #dice metric for boundary difference detection\n",
    "                dice_metric_boundary_difference_detection(\n",
    "                    y_pred = filled_vessels_boundary_deform[0].ne(filled_vessels_boundary[0]),\n",
    "                    y = val_outputs_boundary_deform[0].ne(val_labels_boundary[0])\n",
    "                )\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            #log_metric(\n",
    "            #    'dice',\n",
    "            #    metric,\n",
    "            #    step = epoch\n",
    "            #)\n",
    "            #hausdorf_distance = hausdorf_distance_metric.aggregate().item()\n",
    "            #aggregate the dice score for the prediction fo the deformed raw image againsed the deformed prediction\n",
    "            metric_deform = dice_metric_deform.aggregate().item()\n",
    "            #log_metric(\n",
    "            #    'dice_deformed_validation',\n",
    "            #    metric_deform,\n",
    "            #    step = epoch\n",
    "            #)\n",
    "            metric_deform_boundary_difference = dice_metric_deform_boundary_difference.aggregate().item()\n",
    "            #log_metric(\n",
    "            #    'ground_truths_boundary_dice',\n",
    "            #    metric_deform_boundary_difference,\n",
    "            #    step = epoch\n",
    "            #)\n",
    "            metric_predicted_deform_boundary_difference = dice_metric_predicted_deform_boundary_difference.aggregate().item()\n",
    "            #log_metric(\n",
    "            #    'predicted_boundary_dice',\n",
    "            #    metric_predicted_deform_boundary_difference,\n",
    "            #    step = epoch\n",
    "            #)\n",
    "            metric_boundary_difference_detection = dice_metric_boundary_difference_detection.aggregate().item()\n",
    "            #log_metric(\n",
    "            #    'dice_deformed_boundary_ground_truth_to_prediction',\n",
    "            #    metric_boundary_difference_detection,\n",
    "            #    step = epoch\n",
    "            #)\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "            #hausdorf_distance_metric.reset()\n",
    "            dice_metric_deform.reset()\n",
    "            dice_metric_deform_boundary_difference.reset()\n",
    "            #dice_metric_predicted_deform_boundary_difference.reset()\n",
    "            dice_metric_boundary_difference_detection.reset()\n",
    "\n",
    "            metric_values.append(\n",
    "                metric\n",
    "            )\n",
    "            #hausdorf_distance_values.append(\n",
    "            #    hausdorf_distance\n",
    "            #)\n",
    "            metric_values_deform.append(\n",
    "                metric_deform\n",
    "            )\n",
    "            metric_values_deform_boundary_difference.append(\n",
    "                metric_deform_boundary_difference\n",
    "            )\n",
    "            metric_values_predicted_deform_boundary_difference.append(\n",
    "                metric_predicted_deform_boundary_difference\n",
    "            )\n",
    "            metric_values_boundary_difference_detection.append(\n",
    "                metric_boundary_difference_detection\n",
    "            )\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    directory, \"best_metric_model.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                #f\"\\ncurrent mean hausdorf distance: {hausdorf_distance:.4f}\"\n",
    "                f\"\\ncurrent mean dice of deformed mask to predicted deformed image: {metric_deform:.4f}\"\n",
    "                f\"\\ncurrent mean dice of boundary of ground truth, and  boundary of deformed ground truth: {metric_deform_boundary_difference:.5f}\"\n",
    "                #f\"\\ncurrent mean dice of preficted boundary, and predicted deformed boundary: {metric_predicted_deform_boundary_difference:.5f}\"\n",
    "                f\"\\ncurrent mean dice of the boundary difference due to deformation: {metric_boundary_difference_detection:.5f}\"\n",
    "                #f\"\\nratio of dice of boundary deformation to ground truth on prediction and ground truth:{metric_predicted_deform_boundary_difference/metric_deform_boundary_difference}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                f\"at epoch: {best_metric_epoch}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"train completed, best_metric: {best_metric:.4f} \"\n",
    "    f\"at epoch: {best_metric_epoch}\"\n",
    ")\n",
    "#record_metric(\"best_epoch\",best_metric_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a4342",
   "metadata": {},
   "source": [
    "# Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf847a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Epoch_Average_Loss'] = epoch_loss_values\n",
    "df.to_csv(directory + '/metrics_loss.csv')\n",
    "df = pd.DataFrame()\n",
    "df['Val_Mean_Dice'] = metric_values\n",
    "df['boundary_detection_dice'] = metric_values_boundary_difference_detection\n",
    "df['boundary_difference_dice'] = metric_values_deform_boundary_difference\n",
    "df.to_csv(directory + '/metrics_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca73c3e",
   "metadata": {},
   "source": [
    "# Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.savefig(directory +'ipynb_trial_Val_Loss+Dice.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47279946",
   "metadata": {},
   "source": [
    "# Check best model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(directory, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(val_loader):\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_outputs = sliding_window_inference(\n",
    "            val_data[\"image\"].to(device), roi_size, sw_batch_size, model\n",
    "        )\n",
    "        # plot the slice [:, :, 80]\n",
    "        plt.figure(\"check\", (18, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"image {i}\")\n",
    "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(f\"label {i}\")\n",
    "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(f\"output {i}\")\n",
    "        plt.imshow(torch.argmax(\n",
    "            val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
    "        plt.show()\n",
    "        if i == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a1a1da",
   "metadata": {},
   "source": [
    "# Evaluation on original image spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36985eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(\n",
    "            1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-57, a_max=164,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_ds = Dataset(\n",
    "    data=val_files, transform=val_org_transforms)\n",
    "val_org_loader = DataLoader(val_org_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "    EnsureTyped(keys=\"pred\"),\n",
    "    Invertd(\n",
    "        keys=\"pred\",\n",
    "        transform=val_org_transforms,\n",
    "        orig_keys=\"image\",\n",
    "        meta_keys=\"pred_meta_dict\",\n",
    "        orig_meta_keys=\"image_meta_dict\",\n",
    "        meta_key_postfix=\"meta_dict\",\n",
    "        nearest_interp=False,\n",
    "        to_tensor=True,\n",
    "    ),\n",
    "    AsDiscreted(keys=\"pred\", argmax=True, to_onehot=2),\n",
    "    AsDiscreted(keys=\"label\", to_onehot=2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(directory, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_data[\"pred\"] = sliding_window_inference(\n",
    "            val_inputs, roi_size, sw_batch_size, model)\n",
    "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "        # compute metric for current iteration\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    # aggregate the final mean dice result\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "    # reset the status for next validation round\n",
    "    dice_metric.reset()\n",
    "\n",
    "print(\"Metric on original image spacing: \", metric_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f0e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabfb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12)\n",
    "max_epochs = params[\"max_epochs\"]\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "#hausdorf_distance_values = []\n",
    "metric_values_deform = []\n",
    "metric_values_deform_boundary_difference = []\n",
    "metric_values_predicted_deform_boundary_difference = []\n",
    "metric_values_boundary_difference_detection = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True,to_onehot=3)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=3)])\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "            f\"train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    log_metric('epoch_loss',epoch_loss, step = epoch)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                seed = random.randint(0,10000000)\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (128, 128, 128)\n",
    "                sw_batch_size = 1\n",
    "                val_outputs = sliding_window_inference(\n",
    "                    val_inputs, \n",
    "                    roi_size, \n",
    "                    sw_batch_size, \n",
    "                    model\n",
    "                )\n",
    "                #get prediciton output\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                #deform raw image\n",
    "                deform.set_random_state(seed = seed)\n",
    "                deform_val_inputs = torch.unsqueeze(deform(torch.squeeze(val_inputs),mode='bilinear'),axis=0)\n",
    "                #deform validation output\n",
    "                deform.set_random_state(seed = seed)\n",
    "                deform_val_outputs_gt = deform(val_outputs[0],mode='nearest')\n",
    "                #predict on deformed raw image\n",
    "                deform_val_outputs =sliding_window_inference(\n",
    "                    deform_val_inputs, \n",
    "                    roi_size, \n",
    "                    sw_batch_size, \n",
    "                    model\n",
    "                )   \n",
    "                # get predicted outputs\n",
    "                val_outputs_deform = [post_pred(i) for i in decollate_batch(deform_val_outputs)]\n",
    "                #fill holes in prediction\n",
    "                filled  = [FillHoles(applied_labels=[1],connectivity=2)(i) for i in val_outputs]\n",
    "                filled_deform  = [FillHoles(applied_labels=[1],connectivity=2)(i) for i in val_outputs_deform]\n",
    "                #generated boundaries from predictions\n",
    "                filled_vessels_boundary = [LabelToContour()(label_filter(i)) for i in filled]\n",
    "                filled_vessels_boundary_deform = [LabelToContour()(label_filter(i)) for i in filled_deform]\n",
    "                #get labelled data\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                #print(val_labels.shape)\n",
    "                val_labels_deform = [deform_val_outputs_gt]\n",
    "                #get validation boundaries\n",
    "                val_labels_boundary = [LabelToContour()(i) for i in val_labels]\n",
    "                val_outputs_boundary_deform = [LabelToContour()(i) for i in val_labels_deform]\n",
    "                # compute metric for current iteration\n",
    "                #dice metric for ground truth and prediction\n",
    "                dice_metric(\n",
    "                    y_pred = filled,\n",
    "                    y = val_labels\n",
    "                )\n",
    "                #hausdorf_distance_metric(y_pred=filled, \n",
    "                #                         y=val_labels)\n",
    "                #dice metric for deformed output, and prediction on deformed raw image\n",
    "                dice_metric_deform(\n",
    "                    y_pred = filled_deform,\n",
    "                    y = val_outputs_deform\n",
    "                )\n",
    "                #dice metric for boundary of ground truth and deformed ground truth\n",
    "                #measures how much the boundary was deformed\n",
    "                dice_metric_deform_boundary_difference(\n",
    "                    y_pred = val_outputs_boundary_deform,\n",
    "                    y = filled_vessels_boundary\n",
    "                )\n",
    "                #dice metric for boundary difference detection\n",
    "                dice_metric_boundary_difference_detection(\n",
    "                    y_pred=val_outputs_boundary_deform[0],\n",
    "                    y=filled_vessels_boundary_deform[0]\n",
    "                )\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            log_metric(\n",
    "                'dice',\n",
    "                metric,\n",
    "                step = epoch\n",
    "            )\n",
    "            #hausdorf_distance = hausdorf_distance_metric.aggregate().item()\n",
    "            #aggregate the dice score for the prediction fo the deformed raw image againsed the deformed prediction\n",
    "            metric_deform = dice_metric_deform.aggregate().item()\n",
    "            log_metric(\n",
    "                'dice_deformed_validation',\n",
    "                metric_deform,\n",
    "                step = epoch\n",
    "            )\n",
    "            metric_deform_boundary_difference = dice_metric_deform_boundary_difference.aggregate().item()\n",
    "            log_metric(\n",
    "                'ground_truths_boundary_dice',\n",
    "                metric_deform_boundary_difference,\n",
    "                step = epoch\n",
    "            )\n",
    "            metric_boundary_difference_detection = dice_metric_boundary_difference_detection.aggregate().item()\n",
    "            log_metric(\n",
    "                'dice_deformed_boundary_ground_truth_to_prediction',\n",
    "                metric_boundary_difference_detection,\n",
    "                step = epoch\n",
    "            )\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "            dice_metric_deform.reset()\n",
    "            dice_metric_deform_boundary_difference.reset()\n",
    "            dice_metric_boundary_difference_detection.reset()\n",
    "\n",
    "            metric_values.append(\n",
    "                metric\n",
    "            )\n",
    "            #hausdorf_distance_values.append(\n",
    "            #    hausdorf_distance\n",
    "            #)\n",
    "            metric_values_deform.append(\n",
    "                metric_deform\n",
    "            )\n",
    "            metric_values_deform_boundary_difference.append(\n",
    "                metric_deform_boundary_difference\n",
    "            )\n",
    "            metric_values_boundary_difference_detection.append(\n",
    "                metric_boundary_difference_detection\n",
    "            )\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    directory, \"best_metric_model.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\"\\ncurrent mean dice of deformed mask to predicted deformed image: {metric_deform:.4f}\"\n",
    "                f\"\\ncurrent mean dice of boundary of ground truth, and  boundary of deformed ground truth: {metric_deform_boundary_difference:.5f}\"\n",
    "                f\"\\ncurrent mean dice of the boundary difference due to deformation: {metric_boundary_difference_detection:.5f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                f\"at epoch: {best_metric_epoch}\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
